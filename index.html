<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Synthesizing Complex Visual Reasoning Context Using Language Model">
  <meta name="keywords" content="Multimodal Self-Instruct">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title> Multimodal Self-Instruct: Synthesizing Complex Visual Reasoning Context
    Using Language Modelv</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <!-- <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
/Users/panlu/Library/Mobile Documents/com~apple~CloudDocs/ImageMath/visual-mathqa-server/data_final/images
    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link rel="icon" href="./static/images/ZJU/ZJU.svg">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/leaderboard.css">
  <link rel="stylesheet" href="./static/css/my.css">

  <!-- <link href="https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator_bulma.min.css" rel="stylesheet">
  <script type="text/javascript" src="https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js"></script> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.js"></script>
  <script src="./static/js/bulma-slider.js"></script>
  <script src="./static/js/explorer-index.js"></script>
  <script src="./static/js/question_card.js"></script>
  <script src="./static/js/leaderboard.js"></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <!-- <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a> -->
        <!-- @PAN TODO: consider adding links? -->
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="http://www.dcd.zju.edu.cn/wyweb/">
              <b>DCD</b>
              <p style="font-size:18px; display: inline; margin-left: 5px;"></p>
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title is-bold">
              <span class="model" style="vertical-align: middle">Multimodal Self-Instruct</span>
            </h1>
            <h2 class="subtitle is-3 publication-subtitle">
              Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model
            </h2>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a>Wenqi Zhang</a>,
              </span>
              <span class="author-block">
                <a>Zhenglin Cheng</a>,
              </span>
              <span class="author-block">
                <a>Tony Xia</a>,
              </span>
              <span class="author-block">
                <a>Yuanyu He</a>,
              </span>
              <span class="author-block">
                <a>Mengna Wang</a>,
              </span>
              <span class="author-block">
                <a>Yongliang Shen</a>,
              </span>
              <span class="author-block">
                <a>Zeqi Tan</a>,
              </span>
              <span class="author-block">
                <a>Guiyang Hou</a>,
              </span>
              <span class="author-block">
                <a>Mingqian He</a>,
              </span>
              <span class="author-block">
                <a>Yanna Ma</a>,
              </span>
              <span class="author-block">
                <a>Weiming Lu</a>,
              </span>
              <span class="author-block">
                <a>Yueting Zhuang,
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Zhejiang University</span><br>
            </div>

            <!-- <section> -->
            <!-- <div class="section" id="org-banners" style="display:fle">
              <a href="https://www.ucla.edu/" target="_blank" rel="external">
                  <img class="center-block org-banner" src="static/images/ucla.png" style="height:3em">
              </a>
              <a href="https://www.washington.edu/" target="blank" class="ext-link">
                  <img class="center-block org-banner" src="static/images/uw.png" style="height:3em">
              </a>
              <a href="https://www.microsoft.com/en-us/research/" target="_blank" rel="external">
                  <img class="center-block org-banner" src="static/images/microsoft.png" style="height:3em">
              </a>
            </div> -->
            <!-- </section> -->

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <!-- @PAN TODO: change links -->
                  <!-- tag4 -->
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <!-- <a href="https://lupantech.github.io/papers/arxiv23_mathvista.pdf"
                   class="external-link button is-normal is-rounded is-dark"> -->
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/RuikerLiang/Multimodal-Self-Instruct.github.io"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      <p style="font-size:18px">ü§ó</p>
                      <!-- üîó -->
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>
                <!-- Leaderboard Link. -->
                <span class="link-block">
                  <a href="https://Multimodal-Self-Instruct.github.io/#leaderboard"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <p style="font-size:18px">üèÜ</p>
                    </span>
                    <span>Leaderboard</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container" style="margin-bottom: 2vh;">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            Although most current large multimodal models (LMMs) can already understand photos of
            natural scenes and portraits, their understand
            ing of abstract images, e.g., charts, maps, or
            layouts, and visual reasoning capabilities remains quite rudimentary. They often struggle
            with simple daily tasks, such as reading time
            from a clock, understanding a flowchart, or
            planning a route using a road map. In light
            of this, we design a multi-modal self-instruct,
            utilizing large language models and their code
            capabilities to synthesize massive abstract images and visual reasoning instructions across
            daily scenarios. Our strategy effortlessly creates a multimodal benchmark with 11,193 in-
            structions for eight visual scenarios: charts, tables, simulated maps, dashboards, flowcharts,
            relation graphs, floor plans, and visual puzzles.
            This benchmark, constructed with simple
            lines and geometric elements, exposes the
            shortcomings of most advanced LMMs like
            GPT-4V and Llava in abstract image understanding, spatial relations reasoning, and visual
            element induction. Besides, to verify the quality of our synthetic data, we fine-tune an LMM
            using 62,476 synthetic chart, table and road
            map instructions. The results demonstrate improved chart understanding and map navigation
            performance, and also demonstrate potential
            benefits for other visual reasoning tasks.
          </div>
        </div>
      </div>
    </div>
  </section>
  <section class="section">
    <div class="container" style="margin-top: -150px; margin-bottom: -100px;">
      <div class="columns is-centered m-6">
        <div class="column is-full has-text-centered content">
          <div id="results-carousel" class="carousel results-carousel">
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/examples/newPerformance.png" alt="geometric reasoning" width="200%" />
              </div>
            </div>
            <div class="box m-5">
              <div class="content">
                <div class="has-text-centered">
                  <img src="static/images/examples/ChartExamples/PieChart.png" alt="arithmetic reasoning" width="35%" />
                  <img src="static/images/examples/ChartExamples/LineChart.png" alt="algebraic reasoning" width="55%" />
                </div>
              </div>
            </div>
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/examples/ChartExamples/Table.png" alt="geometric reasoning" width="48%" />
                <img src="static/images/examples/ChartExamples/BarChart.png" alt="logical reasoning" width="50%" />

              </div>
            </div>
            <div class="box m-5">
              <div class="content has-text-centered">
                <div class="images_container_horizontal">
                  <div class="images_container_vertical">
                    <img src="static/images/examples/MapExamples/Connectivity1.png" width="75%" />
                    <img src="static/images/examples/MapExamples/Connectivity2.png" width="80%" />
                  </div>
                  <img src="static/images/examples/MapExamples/RelationGraph.png" alt="geometric reasoning"
                    width="40%" />
                </div>
              </div>
            </div>
            <div class="box m-5">
              <div class="content has-text-centered">
                <div class="images_container_vertical">
                  <div class="images_container_horizontal">
                    <img src="static/images/examples/MapExamples/Map1.png" alt="statistical reasoning" width="30%" />
                    <img src="static/images/examples/MapExamples/Map2.png" width="30%" />
                  </div>
                  <div class="images_container_horizontal">
                    <img src="static/images/examples/MapExamples/Map3.png" width="30%" />
                  </div>
                </div>
              </div>
            </div>
            <div class="box m-5">
              <div class="content has-text-centered">
                <div class="images_container_horizontal">
                  <div class="images_container_vertical">
                    <img src="static/images/examples/Puzzle1.png" alt="geometric reasoning" width="60%" />
                    <img src="static/images/examples/Puzzle12.png" width="70%" />
                  </div>
                  <div class="images_container_vertical">
                    <img src="static/images/examples/Puzzle2.png" alt="geometric reasoning" width="180%" />
                    <img src="static/images/examples/Puzzle22.png" width="100%" />
                  </div>

                </div>
              </div>
            </div>
            <div class="box m-5">
              <div class="content has-text-centered">
                <div class="images_container_horizontal">
                  <img src="static/images/examples/RocketDiagram.png" width="50%" />
                  <img src="static/images/examples/Layout.png" width="50%" />
                </div>
              </div>
            </div>
            <div class="box m-5">
              <div class="content has-text-centered">
                <div class="images_container_horizontal">
                  <img src="static/images/examples/Dashboard1.jpg" alt="geometric reasoning" width="31%" />
                  <img src="static/images/examples/Dashboard2.png" width="30%" />
                </div>
              </div>
            </div>

            <div class="box m-5">
              <div class="content has-text-centered ">
                <div class="images_container_horizontal">
                  <img src="static/images/examples/Instrument2.png" width="50%" />
                  <img src="static/images/examples/Instrument.png" width="40%" />
                </div>
              </div>
            </div>
            <div class="box m-5">
              <div class="content has-text-centered ">
                <div class="images_container_horizontal">
                  <div class="images_container_vertical">
                    <img src="static/images/examples/Workflow.png" alt="geometric reasoning" width="50%" />
                    <img src="static/images/examples/Workflow2.png" width="80%" />
                  </div>
                  <img src="static/images/examples/FlowChart.png" width="50%" />
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
  </section>

  <section class="section">
    <div class="container">

      <div class="columns is-centered">
        <div class="column is-full has-text-centered content">
          <h2 class="title is-3" id="leaderboard_title">Leaderboard on Our Benchmark-11k </h2>
          <p>
            We evaluate the performance of many representative LMMs using our benchmark containing all tasks . We
            observe that for these abstract images, even advanced LMMs like GPT-4V and Claude3 achieved only 49.5% and
            50.1% accuracy on average for all tasks, leaving a significant gap to human-level performance (82.1%).
          </p>
          <div class="content">
            <table id="leaderboard">
              <thead>
                <tr>
                  <th>Rank</th>
                  <th>Name</th>
                  <th>Chart</th>
                  <th>Table</th>
                  <th>Road Map</th>
                  <th>Dashboard</th>
                  <th>Relation Graph</th>
                  <th>Flowchart</th>
                  <th>Visual Puzzles</th>
                  <th>Layout</th>
                  <th><b class="best-score-text">Avg</b></th>
                </tr>
                <tr>
                  <th>-</th>
                  <th>Human*</th>
                  <th>93.5</th>
                  <th>95.1</th>
                  <th>75</th>
                  <th>85.3</th>
                  <th>82.5</th>
                  <th>65.5</th>
                  <th>62.5</th>
                  <th>97.6</th>
                  <th>82.1</th>
                </tr>
              </thead>
              <tbody>
                <!--leaderboard.json-->
              </tbody>

            </table>
          </div>
        </div>
      </div>
  </section>

  <!-- DATASET SECTION -->
  <section class="hero is-light is-small">
    <div class="hero-body has-text-centered">
      <h1 class="title is-1 mathvista">
        <span class="model" style="vertical-align: middle">Dataset</span>
      </h1>
    </div>
  </section>

  <section class="section">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <!-- <div class="column is-full-width has-text-centered"> -->
        <div class="column is-four-fifths">
          <h2 class="title is-3">Introduction</h2>
          <div class="content has-text-justified">
            We identify that current LMMs have a significant gap compared to humans in understanding and visually
            reasoning about abstract images, such as maps, charts, and layouts. Utilizing LLM and code, We design a
            multimodal self-instruct strategy to synthesize a diverse set of abstract images and reasoning instructions,
            providing value data for LMMs. We synthesized a benchmark of 11,193 highquality abstract images, covering
            eight common scenarios. Our benchmark reveals significant deficiencies even in advanced LMMs. Besides, we
            synthesized 62,476 chart and road map instructions for fine-tuning, verifying the effectiveness of the
            synthesized data.
          </div>
          <img src="static/images/examples/Table1.png" width="50%">
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <!-- <div class="column is-full-width has-text-centered"> -->
        <div class="column is-four-fifths">
          <h2 class="title is-3">Approach Overview</h2>
          <div class="content has-text-justified">
            Our multi-modal self-instruct is an LLM-driven data synthesis strategy capable of producing abstract images
            and aligned reasoning instructions for various daily scenarios, including road maps, dashboards, 2D planar
            layouts, charts, relation graphs, flowcharts, and visual puzzles.
          </div>
          <img src="static/images/examples/Data.jpg" width="90%">
          <div class="content has-text-justified">
            Firstly, our strategy can autonomously propose a creative idea for visual scenarios, e.g., using a
            step-by-step flowchart to demonstrate how to attend an academy conference or designing road map . Then it
            generates detailed code to visualize this idea. After synthesizing the desired image, LLMs self-instruct
            multiple high-quality Q&A pairs for this visual content. The entire process is fully completed by the LLM
            with a few demonstrations.

          </div>
          <div class="content has-text-justified">
            We illustrate the entire process of our image-text synthesis, including using road maps for navigation,
            interpreting pie charts, solving visual puzzles, and using operating workflow. For each scenario, we
            synthesize multiple questions, annotated answers, and rationales. For example, in the pie chart case, the
            LLM designs a multi-step math question about the difference between the largest and smallest categories.
          </div>
        </div>
      </div>
  </section>

  <section class="section">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <!-- <div class="column is-full-width has-text-centered"> -->
        <div class="column is-four-fifths">
          <h2 class="title is-3">Fine-Turning Result</h2>
          <div class="content has-text-justified">
            In addition to constructing the benchmark, we fine-tuned the Llava-1.5-7B model using the training sets from
            chart, table, and map tasks, and compared its performance with other baselines
          </div>
          <img src="static/images/examples/Table2.png" width="50%">
          <div class="content has-text-justified">
            We evaluate whether Llava-our-62k can generalize to other benchmarks, especially the tasks with significant
            differences. These results show that our model can generalize to other types of visual reasoning tasks,
            rather than merely fitting to the training scenarios.
          </div>
          <img src="static/images/examples/Table4.png" width="100%">
        </div>
  </section>

  <section class="section">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <!-- <div class="column is-full-width has-text-centered"> -->
        <div class="column is-full-width">
          <h2 class="title is-3">Case</h2>
          <div class="column is-full has-text-centered content">

          </div>
        </div>
      </div>

    </div>
  </section>


  <section class="section">
    <div class="container" style="margin-top: -150px; margin-bottom: -100px;">
      <div class="columns is-centered m-6">
        <div class="column is-full has-text-centered content">
          <div class="box m-5">
            <div class="content has-text-centered ">
              <div class="images_container_horizontal">
                <div class="images_container_vertical">
                  <img src="static/images/examples/Case1.png" width="100%" />
                </div>
              </div>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered ">
              <div class="images_container_horizontal">
                <div class="images_container_vertical">
                  <img src="static/images/examples/Case2.png" width="100%" />
                </div>
              </div>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered ">
              <div class="images_container_horizontal">
                <div class="images_container_vertical">
                  <img src="static/images/examples/Case3.png" width="100%" />
                </div>
              </div>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered ">
              <div class="images_container_horizontal">
                <div class="images_container_vertical">
                  <img src="static/images/examples/Case4.png" width="100%" />
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
  </section>


  <!-- @PAN TODO: bibtex -->
  <!--
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3 has-text-centered">BibTeX</h2>
    <pre><code>@inproceedings{lu2024mathvista,
  author    = {Lu, Pan and Bansal, Hritik and Xia, Tony and Liu, Jiacheng and Li, Chunyuan and Hajishirzi, Hannaneh and Cheng, Hao and Chang, Kai-Wei and Galley, Michel and Gao, Jianfeng},
  title     = {MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts},
  booktitle={International Conference on Learning Representations (ICLR)},
  year      = {2024}
}</code></pre>
  </div>
</section>
-->

  <section>
    <div class="section" id="org-banners" style="display:flex">
      <img class="center-block org-banner" src="static/images/ZJU/ZJU_Big.png">
    </div>
  </section>


  <footer class="footer">
    <!-- <div class="container"> -->
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is website adapted from <a href="https://mathvista.github.io/">MathVista</a> and <a
              href="https://nerfies.github.io/">Nerfies</a>, licensed under a <a rel="license"
              href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
    <!-- </div> -->
  </footer>

</body>

</html>